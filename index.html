<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>LRQ-FACT</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css2?family=Audiowide&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@600&display=swap" rel="stylesheet">

    <style>
        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: Georgia, serif;
            max-width: 1400px;
            margin: 2rem auto;
            padding: 1rem;
            background: #f9f9f9;
            color: #333;
            line-height: 1.7;
        }
        
        .header-wrapper {
            position: sticky;
            top: 0;
            z-index: 999;
            background: rgba(30, 30, 30, 0.9);
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            padding: 1rem 2rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.3);
            transition: padding 0.3s ease;
        }

        .header-wrapper.shrink {
            padding: 0.5rem 2rem;
        }

        .header-title {
            font-family: 'Share Tech Mono', monospace;
            font-size: 3rem;
            text-align: center;
            color: #f5f5f5;
            margin: 0;
            transition: font-size 0.3s ease;
        }

        .header-wrapper.shrink .header-title {
            font-size: 2rem;
        }


        nav {
            display: flex;
            justify-content: center;
            gap: 2rem;
            padding: 0.5rem 0;
        }

        nav a {
            position: relative;
            font-family: 'Share Tech Mono', monospace;
            font-size: 1.25rem;
            color: #a1a1a1;
            text-decoration: none;
            transition: color 0.2s ease;
        }

        nav a::after {
            content: "";
            position: absolute;
            bottom: -5px;
            left: 0;
            height: 2px;
            width: 0%;
            background-color: #2ecc71; /* accent color: emerald green */
            transition: width 0.2s ease-in-out;
        }

        nav a:hover {
            color: #ffffff;
        }

        nav a:hover::after {
            width: 100%;
        }

        .section {
            padding-top: 20px;
            scroll-margin-top: 130px; /* adjust to match header height */
        }

        /* Keep your original styles below this */
        .tcolorbox {
            background-color: #fcfbea;
            border: 1px solid #888;
            border-radius: 8px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
            margin: 1rem 0;
            overflow: hidden;
        }

        .tcolorbox-title {
            background-color: #666;
            color: #fff;
            padding: 0.5rem 1rem;
            font-weight: bold;
            font-variant: small-caps;
        }

        .tcolorbox-content {
            padding: 1rem 1.5rem;
            font-size: 0.95rem;
            color: #333;
            line-height: 1.6;
        }

        .tcolorbox-separator {
            border: none;
            border-top: 1px dashed #888;
            margin: 1rem 0 0.5rem 0;
        }

        .figure-caption {
            text-align: center;
            font-size: 1.1rem;
            color: #444;
            margin-top: -0.5rem;
            margin-bottom: 1rem;
            font-family: serif;
        }

        ol {
            padding-left: 1.2rem;
        }

        em {
            display: block;
        }

        .table-wrapper {
            margin: 2rem 0;
            overflow-x: auto;
        }

        .table-caption {
            font-weight: bold;
            font-size: 1rem;
            margin-bottom: 0.5rem;
            font-family: Georgia, serif;
            color: #444;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }

        .wide-table {
            min-width: 1000px;
            table-layout: fixed;
        }

        table th, table td {
            border: 1px solid #ccc;
            padding: 0.6rem;
            text-align: center;
            vertical-align: middle;
        }

        table th {
            background: #f2f2f2;
        }

        sub {
            font-size: 0.75em;
            color: #888;
        }


    </style>
</head>

<body>
    <div class="header-wrapper">
        <div class="header-title">LRQ-FACT</div>
        <nav>
            <a href="#">Overview</a>
            <a href="#case-study">Case Study</a>
            <a href="#experiments">Experiments</a>
        </nav>
    </div>

    <!-- Section: Overview -->
    <div id="overview" class="section">
        <h1>📊 Overview</h1>
        <img src="figure/LRQ-Fact_Pipeline.jpg"
            alt="LRQ-FACT Pipeline"
            style="display: block; margin: 0 auto; margin-bottom: 0.5rem; width: 65%; border-radius: 6px; border: 20px solid #ffffff;">

        <div class="figure-caption">
            Overview of the LRQ-FACT framework pipeline.
        </div>
        <!-- Optionally add a few paragraphs summarizing the method -->
    </div>

    <!-- Section: Case Study -->
    <div id="case-study" class="section">
        <h1>🧪 Case Study</h1>

        <h2>Real</h2>
        <p>
            In this case study, the description is well-constructed and aligns perfectly with
            the image and the textual context. The depiction of Jake Davis as a young man in casual clothing, standing in a
            relaxed manner, accurately reflects the narrative of his release from a young offender institution. The visual
            context provided in the image adds credibility to the news article, confirming the validity of the description.
            There are no discrepancies between the image and the text, making the description not only good but also a
            reliable tool to confirm the factual correctness of the news.
            <br><br>
            The questions presented in this case are well-formed and reliable. They are designed to extract key details from
            both the image and the text, ensuring comprehensive verification. The visual questions effectively ask about the
            setting and identity, which helps in confirming whether the person and location in the image match the article’s
            claims. The text-based questions aim to validate the timeline and factual details, ensuring a consistent
            narrative. These questions are precise and structured to get the best possible answers, making them a solid
            mechanism for cross-verifying facts.

        </p>

        <!-- Case 1 -->
        <div class="tcolorbox">
            <div class="tcolorbox-content">
                <img src="figure/Example1.jpg" alt="Case 1 Image"
                    style="width:100%; border-radius: 6px; border: 1px solid #aaa;">
                <p>
                    This example case illustrating the alignment between image and text in a fact-checking process. The
                    generated questions verify key elements, ensuring consistency and accuracy in multimodal
                    misinformation detection. This demonstrates how targeted questions and well-constructed descriptions
                    enhance reliable fact-checking outcomes.
                </p>
            </div>
        </div>

        <br>
        <h2>Textual Veracity Distortion</h2>
        <p>
            The description in the following study accurately depicts a lighthouse in a Gothic architectural style,
            positioned on a rocky shore with surrounding water and seagulls. The image is valid and corresponds with the
            article's general theme. However, the description’s alignment with the actual claim in the text—that the
            lighthouse is haunted and located in Greece—proves to be incorrect. While the description is visually consistent
            and good, it does not support the erroneous textual claim, showing how important it is to assess both text and
            visuals in tandem.
            <br><br>
            The questions in this case are reliable and appropriately structured to identify discrepancies between the image
            and the text. The visual questions ask about the architectural style and contextual clues from the image, while
            the text-based questions explore the factual accuracy of the claim that this lighthouse is haunted and located
            in Greece. The questions provide a good framework for fact-checking by encouraging thorough scrutiny of both
            visual and textual elements. This ensures that any distortions or misrepresentations in the article are
            effectively highlighted, making the questions a valuable tool for getting to the truth.
        </p>

        <!-- Case 2 -->
        <div class="tcolorbox">
            <div class="tcolorbox-content">
                <img src="figure/Example4.jpg" alt="Case 2 Image"
                    style="width:100%; border-radius: 6px; border: 1px solid #aaa;">
                <p>
                    This example case illustrating textual veracity distortion. The image description aligns visually with
                    the content, but fails to support the false textual claim about the haunted lighthouse's location in
                    Greece. The generated questions are designed to detect inconsistencies, providing a thorough framework
                    for fact-checking by scrutinizing both visual and textual elements.
                </p>
            </div>
        </div>

        <br>
        <h2>Visual Veracity Distortion</h2>
        <p>
            Here, the description of a clock tower in yellow and white is valid and clear, but the image itself shows a
            structure that is clearly gold and digitally altered. The description is good in terms of clarity and helping
            readers visualize the article's claim, even though it does not reflect the manipulated nature of the image. This
            highlights the importance of analyzing the veracity of visuals alongside textual descriptions.
            <br><br>
            The questions are well-crafted to reveal any visual inconsistencies. The visual questions ask about the color
            and reality of the clock tower, which are key to identifying that the clock tower has been digitally altered.
            The text-based questions, which probe the existence of such a clock tower in real life, also help uncover
            discrepancies. These questions are reliable and precise, aimed at extracting the best possible answers and
            guiding the evaluation of the article’s claims against the evidence provided by the image.
        </p>

        <!-- Case 3 -->
        <div class="tcolorbox">
            <div class="tcolorbox-content">
                <img src="figure/Example3.jpg" alt="Case 3 Image"
                    style="width:100%; border-radius: 6px; border: 1px solid #aaa;">
                <p>
                    This example case highlighting visual manipulation. The description accurately conveys the textual claim
                    about a yellow and white clock tower, but fails to reflect the digitally altered gold structure seen in
                    the image. The questions focus on detecting visual discrepancies, such as the altered colors, and also
                    probe the existence of such a clock tower, providing a reliable framework for evaluating both the image
                    and text.
                </p>
            </div>
        </div>

        <br>
        <h2>Cross-modal Consistency Distortion</h2>
        <p>
            This case involves a clear mismatch between the text and the image, where the
            article describes a little girl holding uncooked rolls, while the image shows her holding paper towels. The
            description is coherent and well-explained, making it a good tool to visualize the scenario presented in the
            article. However, the inconsistency between the image and the text highlights a cross-modal distortion. Despite
            this, the description itself remains valid in its own right.
            <br><br>
            The questions presented are well-designed to highlight the inconsistency between the text and the image. The
            visual questions ask about the scene and the object the girl is holding, providing clear answers that reveal the
            mismatch. The text-based questions further confirm this by addressing the article’s lack of accurate
            description. These questions are well-structured and reliable, allowing for an in-depth examination of both the
            image and the text to expose cross-modal discrepancies. They guide the analysis toward the best possible answers
            by focusing on the key elements that need verification.
        </p>

        <!-- Case 4 -->
        <div class="tcolorbox">
            <div class="tcolorbox-content">
                <img src="figure/Example2.jpg" alt="Case 4 Image"
                    style="width:100%; border-radius: 6px; border: 1px solid #aaa;">
                <p>
                    This example case demonstrating cross-modal distortion. The description is clear and helps visualize the
                    article’s scenario of a little girl holding uncooked rolls, while the image actually shows her holding
                    paper towels. This mismatch between text and image points to a cross-modal distortion. The questions are
                    well-crafted to reveal this inconsistency by focusing on both the scene and the object in the girl's
                    hands, providing a reliable framework for identifying the discrepancy between the article and the image.
                </p>
            </div>
        </div>

        <br><br>
        <hr class="tcolorbox-separator"><br><br>

        <h1>📄 LRQ-FACT: Prompt Templates</h1>

        <h2>📘 Prompt Overview</h2>
        <p>
            The <strong>LRQ-FACT</strong> framework employs a series of structured prompts to guide LLMs and VLMs in
            multimodal fact-checking.
            These prompts facilitate the generation of detailed image descriptions, contextually relevant questions, and
            well-informed answers
            that probe the veracity of both visual and textual content. In the final step, a rule-based decision-maker
            evaluates the generated
            questions and answers to provide a final judgment on the consistency between the text and image, ensuring
            accurate detection of misinformation.
        </p>

        <h2>Image Description Prompt</h2>
        <p>
            The first step is to generate a detailed description of the image,
            capturing all relevant elements that help assess its consistency with the textual content. This description is
            crucial for identifying potential inconsistencies or manipulations between the image and the accompanying
            article.
        </p>

        <!-- Prompt 1: Image Description -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Image Description Prompt:</div>
            <div class="tcolorbox-content">
                <em>
                    Please provide a detailed and comprehensive description of the image shown. Focus on identifying all
                    visible elements including objects, people, setting, and any interactions or actions taking place.
                    Describe the colors, textures, mood, and any other notable aspects that contribute to the overall
                    context and significance of the image.
                </em>
            </div>
        </div>
        <div class="figure-caption">
            Fig. 1: Structured prompt to generate detailed image descriptions.
        </div>

        <h2>Visual Questions Prompt</h2>
        <p>
            This stage generates relevant visual questions designed to verify the
            accuracy, authenticity, and relevance of the visual content in relation to the article. These questions help
            clarify the image content and assess its relation to the text.
        </p>

        <!-- Prompt 2: Visual Questions -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Visual Questions Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    Given the following news article <strong>[news text]</strong>, generate up to <strong>[number of
                        questions]</strong> questions that are directly based on the news article and are designed to
                    explore visual elements that could be present in an image related to the article.
                </p>

                <p><strong>Instructions for Question Generation:</strong></p>
                <em>
                    Focus on generating questions that are directly relevant to the news article and the visual elements
                    that could be present in an image. The questions should examine visible interactions, settings, actions,
                    text, symbols, and specific objects mentioned in the article. Additionally, include questions that
                    assess the authenticity of the image, such as whether it could have been AI-generated or contains any
                    unusual or suspicious elements.
                </em>

                <p><strong>Avoid the following in your Questions:</strong></p>
                <em>
                    - Do not mention any names.<br>
                    - Do not ask questions about identification.<br>
                    - Do not ask about personal details.<br>
                    - Do not ask compound questions in a single sentence.
                </em>

                <p><strong>Example Questions:</strong></p>
                <ol>
                    <li><em>What event is depicted in this image?</em></li>
                    <li><em>How are the people in the image interacting?</em></li>
                    <li><em>Is the person in the image performing [action from article]?</em></li>
                    <li><em>What are the technical aspects or tools used to create this image?</em></li>
                    <li><em>What emotions does this image evoke?</em></li>
                    <li><em>What are the main objects or elements visible in this image?</em></li>
                    <li><em>What unusual elements in the image might suggest digital manipulation or artificial
                            creation?</em></li>
                </ol>
                <hr class="tcolorbox-separator">
                <p><strong>Questions:</strong> 1., 2., ...</p>

            </div>
        </div>
        <div class="figure-caption">
            Fig. 2: Structured prompt to generate relevant visual questions.
        </div>

        <h2>Visual Answers Prompt</h2>
        <p>
            After generating the visual questions, this prompt helps in generating answers that analyze the visual content
            directly from the image. These answers are based on the key elements and actions identified in the image,
            ensuring that the responses are relevant and insightful.
        </p>
        <!-- Prompt 3: Visual Answers -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Visual Answers Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    You are an advanced AI model with access to a vast repository of knowledge and the capability of
                    answering image questions. Your task is to answer the following questions <strong>[generated
                        questions]</strong> based on the image <strong>[image]</strong>. While a news article <strong>[news
                        text]</strong> is provided for context, you must answer the questions solely based on the image and
                    not refer to the article’s content.
                </p>

                <p><strong>Instructions for Answer Generation:</strong></p>
                <em>
                    - Provide accurate, clear, and concise answers to each question.<br>
                    - Your responses should be based entirely on the image.<br>
                    - Do not reference or rely on the content of the provided news article when forming your answers.<br>
                    - Each answer should be directly relevant to the question asked.
                </em>

                <p><strong>Avoid the following in your Answers:</strong></p>
                <em>
                    - Provide accurate, clear, and concise answers to each question.<br>
                    - Your responses should be based entirely on the image.<br>
                    - Do not reference or rely on the content of the provided news article when forming your answers.<br>
                    - Each answer should be directly relevant to the question asked.
                </em>
                <hr class="tcolorbox-separator">
                <p><strong>Answers:</strong> 1., 2., ...</p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 3: Structured prompt to generate answers for the visual questions.
        </div>

        <h2>Textual Questions Prompt</h2>
        <p>
            To critically assess the factual claims in the text, this prompt generates relevant questions targeting specific
            elements such as dates, names, locations, and events. The generated questions aim to challenge the accuracy of
            the claims made in the article.
        </p>
        <!-- Prompt 4: Textual Questions -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Textual Questions Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    Given the following news article <strong>[news text]</strong>, analyze the text and formulate up to
                    <strong>[number of questions]</strong> questions that probe the accuracy and verifiability of the
                    information contained in the article. These questions should be designed to identify potential
                    inaccuracies or areas that can be confirmed or challenged based on general knowledge or the text itself.
                </p>

                <p><strong>Instructions for Question Generation:</strong></p>
                <em>
                    Focus on generating high-quality, fact-checking questions that can be answered directly through general
                    knowledge that an LLM might possess. Identify and question significant factual claims, examine dates,
                    locations, names, and other data mentioned in the article, and challenge any assumptions. The goal is to
                    produce questions that facilitate direct verification of the facts stated in the article.
                </em>

                <p><strong>Aim to Generate:</strong></p>
                <em>
                    - Questions that challenge the accuracy of specific claims made in the article and can be answered based
                    on general knowledge.<br>
                    - Questions that explore potential inconsistencies or contradictions within the article’s content.<br>
                    - Questions that assess the logical coherence and factual basis of the article’s claims.
                </em>

                <p><strong>Avoid asking for:</strong></p>
                <em>
                    - Information requiring external sources or verification beyond general knowledge.<br>
                    - Speculative or opinion-based questions.
                </em>

                <p><strong>Example Questions:</strong></p>
                <ol>
                    <li><em>Does the description of the “meeting between world leaders on March 5th” align with the known
                            schedule of diplomatic events for that time?</em></li>
                    <li><em>Is the account of “a large protest taking place in front of City Hall” consistent with known
                            reports of protests in that area during the stated period?</em></li>
                    <li><em>Does the timeline of “economic sanctions being imposed after the incident” logically follow the
                            typical process for such actions?</em></li>
                    <li><em>Are the historical events referenced, such as “the financial crisis of 2008”, accurately
                            portrayed in the article?</em></li>
                </ol>
                <hr class="tcolorbox-separator">
                <p><strong>Questions:</strong> 1., 2., ...</p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 4: Structured prompt to generate relevant textual questions.
        </div>

        <h2>Textual Answers Prompt</h2>
        <p>
            After generating textual questions, this prompt enables the model to generate answers using its built-in
            knowledge. The specific prompt is shown in Figure 5. Additionally, we employ a
            Retrieval-Augmented Generation (RAG) approach to incorporate factual evidence, ensuring more reliable and
            verifiable responses. These answers help assess factual accuracy and challenge any unsupported claims in the
            article. The corresponding RAG-based prompt is illustrated in Figure 6.
        </p>
        <!-- Prompt 5: Textual Answers (w/o Evidence) -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Textual Answers (w/o Evidence) Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    You are an advanced AI model with access to a vast repository of knowledge. Your task is to answer the
                    following questions <strong>[generated questions]</strong> based on your built-in knowledge. While a
                    news article <strong>[news text]</strong> is provided for context, you must answer the questions solely
                    based on your own knowledge and not refer to the article’s content.
                </p>

                <p><strong>Instructions for Answering:</strong></p>
                <em>
                    Provide accurate, clear, and concise answers to each question. Your responses should be based entirely
                    on your general knowledge and the information you have learned. Do not reference or rely on the content
                    of the provided news article when forming your answers. Each answer should be factually correct and
                    directly relevant to the question asked.
                </em>
                <hr class="tcolorbox-separator">
                <p><strong>Answers:</strong> 1., 2., ...</p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 5: Structured prompt to generate answers based on LLM-knowledge for the relevant textual questions.
        </div>

        <!-- Prompt 6: Textual Answers (w/ Evidence) -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Textual Answers (w/Evidence) Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    You are an advanced AI tasked with evaluating the authenticity of a news article. Your task is to answer
                    the following questions <strong>[generated questions]</strong> based on the provided factual document
                    <strong>[evidence]</strong>. While a news article <strong>[news text]</strong> is provided for context,
                    you must answer the questions solely based on the provided factual document and not refer to the
                    article’s content.
                </p>

                <p><strong>Instructions for Answering:</strong></p>
                <em>
                    Provide accurate, clear, and concise answers to each question. Your responses should be based entirely
                    on the provided factual document. If there was no factual answer for the question use your built-in
                    knowledge to answer the question. Do not reference or rely on the content of the provided news article
                    when forming your answers. Each answer should be directly relevant to the question asked.
                </em>
                <hr class="tcolorbox-separator">
                <p><strong>Answers:</strong> 1., 2., ...</p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 6: Structured prompt to generate answers based on factual evidence for the relevant textual questions.
        </div>

        <h2>Question Quality Assessment Prompt</h2>
        <p>
            To evaluate the relevance of generated questions, we use a fact-checking criteria-based prompt that classifies
            questions as relevant or irrelevant. This assessment considers factors such as alignment with the claim,
            specificity, and usefulness in verifying factual accuracy.
        </p>
        <!-- Prompt 7: Questions Quality -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Questions Quality Prompt:</div>
            <div class="tcolorbox-content">
                <p>
                    You are an advanced AI tasked with evaluating the authenticity of a news article and its accompanying
                    image. Your objective is to determine whether the provided questions <strong>[generated
                        questions]</strong> effectively assess the accuracy, credibility, and reliability of the news
                    article text.
                </p>

                <p><strong>Instructions:</strong><br>
                    <em>Assess each question based on the following expert fact-checking criteria:</em>
                </p>
                <em>
                    1. Critical Thinking and Skepticism: Does the question challenge assumptions, probe deeper into claims,
                    and avoid taking information at face value?<br>
                    2. Analytical Depth: Does it break down complex statements into verifiable components?<br>
                    3. Systematic Approach: Does it follow a structured methodology in assessing sources, claims, and
                    evidence?<br>
                    4. Precision & Specificity: Is it clear, direct, and free from vague or overly broad wording?<br>
                    5. Factual Accuracy: Does it focus on verifying evidence, checking primary sources, and detecting
                    misinformation?<br>
                    6. Logical Consistency: Does it help identify contradictions, misleading narratives, or
                    inconsistencies?<br>
                    7. Source Credibility & Bias Detection: Does it evaluate the reliability of cited sources and potential
                    biases?<br>
                    8. Context Awareness: Does it consider the broader context surrounding the claim?<br>
                    9. Comparative Thinking: Does it encourage cross-referencing with established facts or alternative
                    perspectives?<br>
                    10. Repeatability & Objectivity: Can the question be applied consistently across different cases without
                    personal bias?
                </em>

                <p><strong>Rating Scale:</strong></p>
                <em>
                    - <strong>Relevant</strong>: The question is precise, well-structured, and effectively assesses factual
                    accuracy, credibility, and logical consistency.<br>
                    - <strong>Irrelevant</strong>: The question is vague, lacks depth, or fails to critically probe the
                    credibility and factuality.
                </em>
                <hr class="tcolorbox-separator">
                <p><strong>Answers:</strong> <em>Q1: [Relevant or Irrelevant]<br>
                        Q2: [Relevant or Irrelevant]<br>
                        ...</em></p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 7: Structured prompt to evaluate the quality of generated questions.
        </div>

        <h2>Rule-Based Decision-Maker Prompt</h2>
        <p>
            After gathering information from the image and text analyses, the rule-based decision-maker evaluates the
            consistency between modalities and makes a final determination about the article’s veracity. This module
            provides a detailed explanation for the final judgment.
        </p>
        <!-- Prompt 8: Rule-Based Decision-Maker -->
        <div class="tcolorbox">
            <div class="tcolorbox-title">Rule-Based Decision-Maker Prompt:</div>
            <div class="tcolorbox-content">
                <p>Your objective is to determine whether the article and image are real or fake by analyzing the following
                    information:</p>
                <p>
                    1. News Article Text: <strong>[news text]</strong><br><br>
                    2. Image Description: <strong>[image description]</strong><br>
                    <span style="font-size: 0.95em; color: #333;"><em>Note: This description helps verify consistency with
                            the news text. It is generally reliable but may contain minor discrepancies, such as using
                            different terms like “ocean” instead of “water”.</em></span><br>
                    3. Generated Visual Questions and Answers: <strong>[generated visual FCQs]</strong><br>
                    <span style="font-size: 0.95em; color: #333;"><em>Note: These answers were generated by an AI and may
                            contain mistakes, such as incorrect details regarding locations, names, dates, or objects. They
                            might also incorrectly suggest that the image has been manipulated or is AI-generated. If the
                            answers suggest manipulation or that the image is AI-generated, this should have very low effect
                            on your final decision, especially if the image description and news article text do not contain
                            such indications.</em></span><br>
                    4. Generated Textual Questions and Answers: <strong>[generated textual FCQs]</strong><br>
                    <span style="font-size: 0.95em; color: #333;"><em>Note: These are based on the knowledge of GPT-4O,
                            which is generally reliable but prone to hallucinations or contradictions with other provided
                            information.</em></span>
                </p>

                <p><strong>Instructions:</strong></p>
                <em>
                    To make an accurate judgment of the multimodal misinformation, please follow these steps:<br>
                    <strong>Step 1.</strong> Is there any credible objective evidence refuting the news description? If yes,
                    assign the label: <strong>Textual Veracity Distortion</strong>. If no, continue to Step 2.<br>
                    <strong>Step 2.</strong> Is there any credible objective evidence refuting the news image? If yes,
                    assign the label: <strong>Visual Veracity Distortion</strong>. If no, continue to Step 3.<br>
                    <strong>Step 3.</strong> Does the news caption match the content of the news image? If no, assign the
                    label: <strong>Mismatch</strong>. If yes, and none of the above applies, assign the label:
                    <strong>Real</strong>.
                </em>

                <p><strong>Additional Guidelines:</strong></p>
                <em>
                    1. Assess Overall Consistency: ...<br>
                    2. Examine Details: ...<br>
                    3. Analyze Facial Expressions and Body Language: ...<br>
                    4. Identify Unrealistic Elements: ...<br>
                    5. Cross-Modal Consistency: ...<br>
                    6. Final Judgment: ...<br>
                    7. Select the Most Relevant Label: ...<br>
                    8. Provide a Detailed Explanation: ...
                </em>

                <p><strong>Example Output:</strong></p>
                <em>
                    <strong>1. Final Judgment:</strong> Fake<br>
                    <strong>2. Label:</strong> Visual Veracity Distortion<br>
                    <strong>3. Explanation:</strong> The image description mentions a “cat with pink eyes”, which is highly
                    unnatural and
                    suggests the image is AI-generated. Additionally, ...
                </em>
                <hr class="tcolorbox-separator">
                <p><strong>1. Final Judgment:</strong> [Real or Fake]<br>
                    <strong>2. Label:</strong> [Select one: Textual Veracity Distortion, Visual Veracity Distortion,
                    Mismatch, Real]<br>
                    <strong>3. Explanation:</strong> [Provide your explanation here]
                </p>
            </div>
        </div>

        <div class="figure-caption">
            Fig. 8: Structured prompt to make the final decisions and provide an explanation.
        </div>
    </div>

    <!-- Section: Experiments -->
    <div id="experiments" class="section">
        <h1>🔬 Experiments</h1>

        <div class="table-wrapper">
            <p class="table-caption">
                Table 1: Performance comparison of article-driven versus image-driven visual FCQ generation on MMFakeBench.
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Visual FCQ Generation Method</th>
                        <th>F1</th>
                        <th>Accuracy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Using Image for Question Generation</td>
                        <td>62.2</td>
                        <td>60.1</td>
                    </tr>
                    <tr>
                        <td><strong>Using News Text for Question Generation</strong></td>
                        <td><strong>71.6</strong></td>
                        <td><strong>70.8</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="table-wrapper">
            <p class="table-caption">
                Table 2: Fleiss' Kappa score between human and LLM evaluations of question relevancy.
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Questions / Dataset</th>
                        <th>MMFakeBench</th>
                        <th>DGM4</th>
                        <th>Factify</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Textual</td>
                        <td>0.78</td>
                        <td>0.83</td>
                        <td>0.80</td>
                    </tr>
                    <tr>
                        <td>Visual</td>
                        <td>0.79</td>
                        <td>0.82</td>
                        <td>0.81</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="table-wrapper">
            <p class="table-caption">
                Table 3: Performance of vision-language models and LRQ-FACT across datasets.
            </p>
            <table class="wide-table">
                <thead>
                    <tr>
                        <th rowspan="2">Backbone</th>
                        <th rowspan="2">Approach</th>
                        <th colspan="2">MMFakeBench</th>
                        <th colspan="2">DGM4</th>
                        <th colspan="2">Factify</th>
                    </tr>
                    <tr>
                        <th>F1↑</th><th>ACC↑</th>
                        <th>F1↑</th><th>ACC↑</th>
                        <th>F1↑</th><th>ACC↑</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>VILA</td><td>SP</td><td>11.5</td><td>30.0</td><td>19.4</td><td>19.8</td><td>25.6</td><td>27.7</td></tr>
                    <tr><td>InstructBLIP</td><td>SP</td><td>13.7</td><td>28.8</td><td>19.2</td><td>19.8</td><td>24.3</td><td>26.9</td></tr>
                    <tr><td>BLIP-2</td><td>SP</td><td>16.7</td><td>32.8</td><td>18.1</td><td>19.1</td><td>26.6</td><td>28.6</td></tr>
                    <tr><td>LLaVA-1.6</td><td>SP</td><td>25.7</td><td>40.4</td><td>32.5</td><td>39.4</td><td>51.3</td><td>56.2</td></tr>
                    <tr><td>GPT-4V-1.7T</td><td>SP</td><td>51.0</td><td>54.0</td><td>42.3</td><td>51.5</td><td>64.2</td><td>68.2</td></tr>
                    <tr><td>GPT-4o</td><td>SP</td><td>49.2</td><td>60.9</td><td>39.9</td><td>55.9</td><td><u>72.5</u></td><td><u>71.2</u></td></tr>
                    <tr><td><strong>LRQ-FACT (w/o RAG)</strong></td><td>FCQs</td><td><u>66.5</u></td><td><u>65.5</u></td><td><u>45.8</u></td><td><u>58.0</u></td><td>–</td><td>–</td></tr>
                    <tr><td><strong>LRQ-FACT (w/ RAG)</strong></td><td>FCQs</td><td><strong>71.6</strong></td><td><strong>70.8</strong></td><td><strong>49.2</strong></td><td><strong>62.3</strong></td><td><strong>75.2</strong></td><td><strong>73.1</strong></td></tr>
                </tbody>
            </table>
        </div>

        <div class="table-wrapper">
            <p class="table-caption">
                Table 4: Performance using irrelevant vs. relevant FCQs on MMFakeBench.
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>F1</th>
                        <th>Accuracy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>GPT-4o (No Questions)</td><td>49.2</td><td>60.9</td></tr>
                    <tr><td>LRQ-FACT (Random Questions + RAG)</td><td>67.6</td><td>65.9</td></tr>
                    <tr><td><strong>LRQ-FACT (Relevant Questions + RAG)</strong></td><td><strong>71.6</strong></td><td><strong>70.8</strong></td></tr>
                </tbody>
            </table>
        </div>
    </div>


    <div class="table-wrapper">
        <p class="table-caption">
          Table 5: Ablation study results. Subscripts indicate % improvement over GPT-4o baseline.
        </p>
        <table class="wide-table">
          <thead>
            <tr>
              <th rowspan="2">Method</th>
              <th colspan="2">MMFakeBench</th>
              <th colspan="2">DGM4</th>
              <th colspan="2">Factify</th>
            </tr>
            <tr>
              <th>F1</th><th>ACC</th>
              <th>F1</th><th>ACC</th>
              <th>F1</th><th>ACC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>LRQ-FACT (Paligemma + LLaMA 3.1)</strong></td><td>51.2</td><td>56.4</td><td>41.2</td><td>53.6</td><td>66.2</td><td>65.3</td>
            </tr>
            <tr><td>&emsp;w/ Textual FCQs</td><td>62.5<sub>+22.1%</sub></td><td>59.1<sub>+4.8%</sub></td><td>43.3<sub>+5.1%</sub></td><td>54.1<sub>+0.9%</sub></td><td>64.8<sub>-2.1%</sub></td><td>60.9<sub>-6.7%</sub></td></tr>
            <tr><td>&emsp;w/ Visual FCQs (w/o RAG)</td><td>59.4<sub>+16.0%</sub></td><td>57.3<sub>+1.6%</sub></td><td>46.1<sub>+11.8%</sub></td><td>53.9<sub>+0.5%</sub></td><td>–</td><td>–</td></tr>
            <tr><td>&emsp;w/ Visual &amp; Textual FCQs (w/o RAG)</td><td>62.8<sub>+22.6%</sub></td><td>61<sub>+8.1%</sub></td><td>47.7<sub>+15.7%</sub></td><td>54.8<sub>+2.2%</sub></td><td>–</td><td>–</td></tr>
            <tr><td>&emsp;w/ Textual FCQs (w/ RAG)</td><td>61.7<sub>+20.5%</sub></td><td>63.2<sub>+12.1%</sub></td><td>48.3<sub>+17.2%</sub></td><td>61.7<sub>+15.1%</sub></td><td>69.5<sub>+5.0%</sub></td><td>67.2<sub>+2.9%</sub></td></tr>
            <tr><td>&emsp;w/ Visual &amp; Textual FCQs (w/ RAG)</td><td>64.2<sub>+25.3%</sub></td><td>64.8<sub>+14.8%</sub></td><td>48.6<sub>+17.9%</sub></td><td>62.1<sub>+15.8%</sub></td><td>73.2<sub>+10.5%</sub></td><td>71.5<sub>+9.4%</sub></td></tr>
            <tr><td><strong>LRQ-FACT (GPT-4o)</strong></td><td>49.2</td><td>60.9</td><td>39.9</td><td>55.9</td><td>72.5</td><td>71.2</td></tr>
            <tr><td>&emsp;w/ Textual FCQs</td><td>64.3<sub>+30.7%</sub></td><td>62.1<sub>+2.0%</sub></td><td>40.4<sub>+1.3%</sub></td><td>54.3<sub>-2.9%</sub></td><td>71.1<sub>-1.9%</sub></td><td>68.7<sub>-3.5%</sub></td></tr>
            <tr><td>&emsp;w/ Visual FCQs (w/o RAG)</td><td>59.6<sub>+21.1%</sub></td><td>61.7<sub>+1.3%</sub></td><td>47.2<sub>+18.3%</sub></td><td>59.7<sub>+6.8%</sub></td><td>–</td><td>–</td></tr>
            <tr><td>&emsp;w/ Visual &amp; Textual FCQs (w/o RAG)</td><td><u>66.5</u><sub>+35.2%</sub></td><td><u>65.5</u><sub>+7.5%</sub></td><td>45.8<sub>+14.8%</sub></td><td>58.0<sub>+3.8%</sub></td><td>–</td><td>–</td></tr>
            <tr><td>&emsp;w/ Textual FCQs (w/ RAG)</td><td>61.8<sub>+25.2%</sub></td><td>64.7<sub>+6.2%</sub></td><td><strong>49.4</strong><sub>+23.8%</sub></td><td><strong>62.5</strong><sub>+11.8%</sub></td><td><u>74.0</u><sub>+2.1%</sub></td><td><u>72.5</u><sub>+1.8%</sub></td></tr>
            <tr><td>&emsp;w/ Visual &amp; Textual FCQs (w/ RAG)</td><td><strong>71.6</strong><sub>+45.5%</sub></td><td><strong>70.8</strong><sub>+16.3%</sub></td><td><u>49.2</u><sub>+23.3%</sub></td><td><u>62.3</u><sub>+11.5%</sub></td><td><strong>75.2</strong><sub>+3.7%</sub></td><td><strong>73.1</strong><sub>+2.7%</sub></td></tr>
          </tbody>
        </table>
      </div>
      

    



</body>

<script>
    const headerWrapper = document.querySelector('.header-wrapper');
    window.addEventListener('scroll', () => {
        if (window.scrollY > 50) {
            headerWrapper.classList.add('shrink');
        } else {
            headerWrapper.classList.remove('shrink');
        }
    });
</script>

</html>